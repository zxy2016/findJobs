# 项目开发计划 (Project Development Plan)

本文档用于跟踪“FindJobs AI 助手”项目的开发进度。所有开发活动都将遵循此计划，并在每个步骤完成后更新此文档。

---

## 第一阶段：数据基础与爬虫框架

**目标:** 构建一个稳定、可扩展的爬虫服务以及用于存储岗位数据的数据库基础。

**阶段完成状态: [ 已完成 ]**

### 1.1 项目初始化
- [x] 创建 `requirements.txt` 文件，并写入初始依赖。
- [x] 创建 `.env.example` 文件，作为环境变量配置的模板。
- [x] 创建项目核心目录结构 (`app`, `scraper`, `core` 等)。
- [x] 创建 `README.md` (中文版)。
- [x] 创建 `DESIGN.md` (中文版)。
- [x] 创建 `DEVELOPMENT_PLAN.md` (本文件)。

### 1.2 数据库设置
- [x] **1.2.1**: 在 `app/models/` 中定义 `Job` 数据库模型 (`job.py`)。
- [x] **1.2.2**: 在 `app/db/` 中设置数据库引擎和会话管理 (`session.py`)。
- [x] **1.2.3**: 编写一个初始化脚本 (`init_db.py`)，用于创建数据库和表。

### 1.3 爬虫核心设计
- [x] **1.3.1**: 在 `app/scraper/` 中设计并实现一个抽象基类 `BaseScraper` (`base.py`)。
- [x] **1.3.2**: 在 FastAPI 中添加 API 端点 (`/api/v1/scrape/{site_name}`) 来触发爬虫。

### 1.4 海尔招聘爬虫实现
- [x] **1.4.1**: 创建 `HaierScraper` 类，实现职位列表的 API 抓取。
- [x] **1.4.2**: 实现详情页详细信息（职责、要求等）的抓取。
- [x] **1.4.3**: 实现高效的增量爬取逻辑，避免全量重复抓取。

### 1.5 测试 (第一阶段)
- [x] **1.5.1**: (单元测试) 编写测试用例验证 `Job` 模型的字段和类型。
- [x] **1.5.2**: (集成测试) 通过手动执行和日志观察，验证了端到端爬取流程的正确性。

---

## 第二阶段：前端管理界面

**目标:** 开发用于岗位管理和爬虫管理的前端界面。

**阶段完成状态: [ 已完成 ]**

### 2.1 前端项目初始化
- [x] **2.1.1**: 使用 `create-react-app` 初始化 `frontend` 目录。
- [x] **2.1.2**: 安装 `antd`, `axios`, `react-router-dom` 等核心依赖。
- [x] **2.1.3**: 创建 `pages`, `components`, `api` 等基础目录结构。

### 2.2 基础框架搭建
- [x] **2.2.1**: 创建空的 `JobsManagement` 和 `CrawlerManagement` 页面组件。
- [x] **2.2.2**: 配置 React Router，实现基础的页面导航菜单和路由功能。
- [x] **2.2.3**: 在 FastAPI 后端配置 CORS，允许前端应用跨域访问。
- [x] **2.2.4 (测试)**: 启动前后端服务，验证页面可以正常访问和切换，无跨域错误。

### 2.3 爬虫管理页面开发
- [x] **2.3.1**: 使用 Ant Design 的 `Card` 和 `Button` 组件，静态展示“海尔招聘”爬虫。
- [x] **2.3.2**: 实现点击按钮调用后端 API 的功能。
- [x] **2.3.3**: 实现触发任务后的加载状态和反馈提示（包括状态轮询）。
- [x] **2.3.4 (测试)**: 在页面上点击按钮，确认后端能收到请求并实时反馈任务状态。

### 2.4 岗位管理页面开发
- [x] **2.4.1 (后端)**: 开发 `GET /api/v1/jobs` API，支持分页、排序和多维度筛选。
- [x] **2.4.2 (前端)**: 使用 Ant Design 的 `Table` 组件，从后端获取并展示岗位列表。
- [x] **2.4.3 (前端)**: 实现表格的分页和排序功能。
- [x] **2.4.4 (前端)**: 实现关键词搜索和多条件筛选功能（包括下拉框搜索和重置）。
- [x] **2.4.5 (前端)**: 实现可拖拽调整列宽的功能。
- [x] **2.4.6 (前端)**: 实现点击表格行，通过抽屉(Drawer)展示岗位全部详情的功能。
- [x] **2.4.7 (测试)**: 全面测试岗位管理页面的所有功能。

---

## 第三阶段：个人分析与AI集成

**目标:** 实现用户提交个人信息（简历、文本）的功能，并调用 LLM 服务完成个人画像的结构化分析。

**阶段完成状态: [ 未开始 ]**

- [ ] **3.1 数据库模型设计:**
  - [ ] 在 `app/models/` 中创建 `User` 和 `UserProfile` 的 SQLAlchemy 模型。
  - [ ] 更新 `init_db.py` 脚本以包含新表的创建。

- [ ] **3.2 LLM 客户端设计:**
  - [ ] 在 `app/core/` 中设计一个灵活的 `LLMClient` 基类，用于未来适配不同的大模型服务商。
  - [ ] 实现一个具体的 `GeminiClient`，用于调用 Gemini API。

- [ ] **3.3 简历解析后端开发:**
  - [ ] 安装 `pypdf` 和 `python-docx` 库。
  - [ ] 创建 `POST /api/v1/profile/upload` API 端点，用于接收简历文件。
  - [ ] 实现文件解析、文本提取、构建简历分析 Prompt 的核心逻辑。
  - [ ] 实现调用 LLM 服务，并将返回的结构化 JSON 保存到 `user_profiles` 表的功能。

- [ ] **3.4 前端上传界面:**
  - [ ] 在前端创建一个新的“个人分析”页面。
  - [ ] 使用 Ant Design 的 `Upload` 组件，实现文件上传交互。
  - [ ] 上传成功后，调用后端 API，并以格式化的形式展示返回的 JSON 个人画像。

- [ ] **3.5 (测试):**
  - [ ] 为简历解析 API 编写后端单元测试。
  - [ ] 在前端手动上传简历，端到端验证整个解析和展示流程。

---

## 第四阶段：智能匹配与推荐

**目标:** 基于用户画像和岗位数据，实现智能匹配，并通过 API 提供推荐结果。

**阶段完成状态: [ 未开始 ]**

- ... (详情略)

---

## 第五阶段：优化与部署准备

**目标:** 优化系统，并将爬虫任务自动化，为最终部署做准备。

**阶段完成状态: [ 未开始 ]**

- ... (详情略)